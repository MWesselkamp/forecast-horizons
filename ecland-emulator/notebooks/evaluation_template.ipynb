{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "276d258e",
   "metadata": {},
   "source": [
    "# Evaluating ECLand Emulator on ISMN data\n",
    "\n",
    "In adjusting the flags, we can choose the network, station, soil variable and layer for evaluation. Here we run this example with soil temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157a4b6f-80b5-476b-9369-1efb478626af",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'myevalenv (Python 3.12.6)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "parent_dir = os.path.abspath('..')\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from data_module import *\n",
    "from evaluation_module import *\n",
    "from forecast_module import *\n",
    "from observation_module import *\n",
    "from visualisation_module import *\n",
    "\n",
    "from helpers import *\n",
    "from tests.test_model import *\n",
    "\n",
    "\n",
    "set_global_seed(42)\n",
    "\n",
    "SCRIPT_DIR = os.getcwd()\n",
    "sys.path.append(os.path.dirname(SCRIPT_DIR))\n",
    "print(SCRIPT_DIR) \n",
    "\n",
    "path_to_plots = '../plots/'\n",
    "path_to_results = '../results/'\n",
    "\n",
    "#EX_CONFIG = load_config(config_path = '../../configs/smosmania_st.yaml')\n",
    "EX_CONFIG = load_config(config_path = '../../configs/tereno_st.yaml')\n",
    "\n",
    "network =  EX_CONFIG['network'] #'soil_TERENO_ISMN_2022.nc'#'soil_SMOSMANIA_ISMN_2022.nc' # 'soil_TERENO_ISMN_2022.nc'\n",
    "network_name = network.split('_')[1]\n",
    "station = EX_CONFIG['station'] # 'Lahas'\n",
    "variable = EX_CONFIG['variable'] \n",
    "depth = EX_CONFIG['depth']  # [0.05, 0.2, 0.3]\n",
    "\n",
    "years = EX_CONFIG['years']\n",
    "models = EX_CONFIG['models']# , 'xgb'\n",
    "\n",
    "maximum_leadtime = EX_CONFIG['maximum_leadtime'] # medium range, ten days\n",
    "tolerance = EX_CONFIG['tolerance']\n",
    "score = EX_CONFIG['score']\n",
    "\n",
    "print(\"Network: \", network)\n",
    "print(\"Station: \", station)\n",
    "print(\"Variable: \", variable)\n",
    "print(\"Depth: \", depth)\n",
    "print(\"Years: \", years)\n",
    "print(\"Models: \", models)\n",
    "print(\"Initial time: \", EX_CONFIG['initial_time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04cda1a",
   "metadata": {},
   "source": [
    "To explore station data and ECLand forecasts, we load the untransformed data and plot it. ECLand slightly overestimates initial soil conditions in January, but most notably is the larger variance compared to the measurements in the first layer. From the dataset we loaded, we select the same global mean and standard deviation from the synthetic data to also z-score the station data. Then we explore their overlay. \n",
    "After, we create a hybrid data set with the z-scored station data for soil temperature, that we use for the prognostic initialsation - instead of the ECLand simulations as before. Then we initialise the model with the hybrid data and run the forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f7c805",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'myevalenv (Python 3.12.6)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "Gevenich = ObservationModule(network = network, \n",
    "                             station = station,\n",
    "                             variable = variable,\n",
    "                             depth=depth) # Initialise the Observation Module with the default Station (Gevenich)\n",
    "\n",
    "Gevenich.load_station(years = years) # Load two years of station data for lookback slicing\n",
    "Gevenich.load_forcing() # Load forcing for matching data base with station data\n",
    "closest_grid_cell = Gevenich.match_station_with_forcing() # Match the station with clostest grid cell and extract the index of the grid cell\n",
    "Gevenich.process_station_data() # specify path_to_plots, if you want to visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613a6351",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'myevalenv (Python 3.12.6)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "dynamic_features_dict = {}\n",
    "dynamic_features_prediction_dict = {}\n",
    "\n",
    "for mod in models:\n",
    "\n",
    "    # initialise experiement and setup model with config file\n",
    "    #CONFIG, HPARS, ForecastModel = setup_experiment(model = mod)\n",
    "    if mod == 'mlp':\n",
    "        print('mlp')\n",
    "        CONFIG = load_config(config_path = '../../configs/mlp_emulator.yaml')\n",
    "        HPARS = load_hpars(use_model = '../mlp')\n",
    "        ForecastModel = ForecastModuleMLP(hpars=HPARS, config=CONFIG)    \n",
    "    elif mod == 'lstm':\n",
    "        CONFIG = load_config(config_path = '../../configs/lstm_emulator.yaml')\n",
    "        HPARS = load_hpars(use_model = '../lstm')\n",
    "        ForecastModel = ForecastModuleLSTM(hpars=HPARS, config=CONFIG)\n",
    "    elif mod == 'xgb':\n",
    "        CONFIG = load_config(config_path = '../../configs/xgb_emulator.yaml')\n",
    "        HPARS = None\n",
    "        ForecastModel = ForecastModuleXGB(hpars=HPARS, config=CONFIG)\n",
    "\n",
    "    CONFIG['x_slice_indices'] = closest_grid_cell # adjust the index of the grid cell in the config file before initialising the models\n",
    "\n",
    "    dataset = ForecastModel.initialise_dataset()\n",
    "    model = ForecastModel.load_model()\n",
    "    x_static, x_met, y_prog, y_prog_initial_state = ForecastModel.load_test_data(dataset)  \n",
    "\n",
    "    station_data = Gevenich.slice_station_data(lookback=CONFIG[\"lookback\"],\n",
    "                                t_0=EX_CONFIG['initial_time'])\n",
    "    matching_indices = Gevenich.match_indices(dataset=dataset,\n",
    "                                              target_variables=EX_CONFIG['targets_eval'])\n",
    "\n",
    "    y_prog_initial_state[..., matching_indices] = station_data[:y_prog_initial_state.shape[0]]\n",
    "    \n",
    "    matching_indices = Gevenich.match_indices(dataset=dataset,\n",
    "                                              target_variables=EX_CONFIG['targets_prog'])\n",
    "    print(\"MATCHING INDICES: \", matching_indices)\n",
    "    initial_vector =  Gevenich.transform_station_data(station_data = y_prog_initial_state, \n",
    "                                                  target_variable_list = EX_CONFIG['targets_prog'])\n",
    "\n",
    "    dynamic_features, dynamic_features_prediction = ForecastModel.run_forecast(initial_conditions=initial_vector)\n",
    "    dynamic_features, dynamic_features_prediction = ForecastModel.backtransformation()\n",
    "\n",
    "    dynamic_features_dict[mod] = dynamic_features\n",
    "    dynamic_features_prediction_dict[mod] = dynamic_features_prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cab0a28",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'myevalenv (Python 3.12.6)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "matching_indices = Gevenich.match_indices(dataset=dataset,\n",
    "                                              target_variables=EX_CONFIG['targets_eval'])\n",
    "\n",
    "GevenichPlots = VisualisationModule(network = network,\n",
    "                                    station = station,\n",
    "                                    variable = variable,\n",
    "                                    maximum_leadtime=maximum_leadtime,\n",
    "                                    doy_vector = Gevenich.doy_vector,\n",
    "                                    evaluation = \"poi\", # ens\n",
    "                                    path_to_plots=path_to_plots)\n",
    "\n",
    "GevenichPlots.plot_station_data(dynamic_features_dict['mlp'], station_data, \n",
    "                                matching_indices= matching_indices)\n",
    "GevenichPlots.plot_station_data_and_forecast(dynamic_features_dict, dynamic_features_prediction_dict, station_data,\n",
    "                                matching_indices= matching_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a08d6e",
   "metadata": {},
   "source": [
    "## Evaluate forecast with relative skill\n",
    "\n",
    "Then we evaluate both ECLand and AIland forecast:\n",
    "1) Directly against the station data: $D(y_{ai}, y)$ and $D(y_{ec}, y)$\n",
    "2) Then both against the observed climatology: $D(y_{ai}, y) / D(y_c, y)$ and $D(y_{ec}, y) / D(y_c, y)$\n",
    "\n",
    "Dependent on what we want to report, we look at the ensemble scores over time, or at the spatially aggregated scores. D can here be the MSE, such that with 2) we compute the MSESS. It could also be the ACC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9ddee0",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'myevalenv (Python 3.12.6)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def get_daily_forecasts(data):    \n",
    "    return data[0::4], data[1::4] , data[2::4], data[3::4]\n",
    "\n",
    "fc_numerical = dynamic_features_dict['mlp']\n",
    "fc_emulators = dynamic_features_prediction_dict\n",
    "\n",
    "station_data_1, station_data_2, station_data_3, station_data_4 = get_daily_forecasts(station_data)\n",
    "fc_numerical_1, fc_numerical_2, fc_numerical_3, fc_numerical_4 = get_daily_forecasts(dynamic_features_dict['mlp'])\n",
    "\n",
    "fc_emulators_1 = {}\n",
    "fc_emulators_2 = {}\n",
    "fc_emulators_3 = {}\n",
    "fc_emulators_4 = {}\n",
    "\n",
    "for mod, fc in dynamic_features_prediction_dict.items():\n",
    "\n",
    "    output1, output2, output3, output4 = get_daily_forecasts(fc)\n",
    "\n",
    "    fc_emulators_1[mod] = output1\n",
    "    fc_emulators_2[mod] = output2\n",
    "    fc_emulators_3[mod] = output3\n",
    "    fc_emulators_4[mod] = output4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48edb12f",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'myevalenv (Python 3.12.6)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "matching_indices = Gevenich.match_indices(dataset=dataset,\n",
    "                                              target_variables=EX_CONFIG['targets_eval'])\n",
    "\n",
    "layers = {}\n",
    "for layer in [0,1,2]:\n",
    "    EvaluateModel = EnsembleEvaluation(score =  score,\n",
    "                                        layer_index = layer,\n",
    "                                        variable_indices = matching_indices,\n",
    "                                        maximum_evaluation_time = maximum_leadtime)\n",
    "\n",
    "    EvaluateModel.set_samples(observations=station_data,\n",
    "                                fc_numerical=fc_numerical,\n",
    "                                fc_emulator=fc_emulators)\n",
    "    EvaluateModel.subset_samples()\n",
    "    ensemble_score = EvaluateModel.evaluate_emulator()\n",
    "    numerical_score = EvaluateModel.evaluate_numerical()\n",
    "    ensemble_skill = EvaluateModel.get_skill_score()\n",
    "\n",
    "    scores = {}\n",
    "    scores_dispersion = {}\n",
    "    skill_scores = {}\n",
    "    scores[\"ECLand\"] = numerical_score\n",
    "    scores[\"Emulators\"] = ensemble_score[0]\n",
    "    scores_dispersion[\"Emulators\"] = ensemble_score[1]\n",
    "    skill_scores[\"Emulators\"] = ensemble_skill\n",
    "\n",
    "    layers[f\"layer{layer}\"] = {}\n",
    "    layers[f\"layer{layer}\"][\"scores\"] = scores\n",
    "    layers[f\"layer{layer}\"][\"skill_scores\"] = skill_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc22e6fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'myevalenv (Python 3.12.6)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "EnsemblePlots = VisualisationModule(network = network,\n",
    "                                    station = station,\n",
    "                                    variable = variable,\n",
    "                                    maximum_leadtime=maximum_leadtime,\n",
    "                                    doy_vector = Gevenich.doy_vector,\n",
    "                                    evaluation = \"ens\", # ens\n",
    "                                    path_to_plots=path_to_plots)\n",
    "\n",
    "EnsemblePlots.plot_scores(layers['layer0']['scores'], layers['layer1']['scores'], layers['layer2']['scores'],\n",
    "                          score = \"MAE\", log_y=False)\n",
    "EnsemblePlots.plot_skill_scores(layers['layer0']['skill_scores'], layers['layer1']['skill_scores'], layers['layer2']['skill_scores'],\n",
    "                          score = \"MAE\", log_y=False, sharey = False, invert=True)\n",
    "EnsemblePlots.plot_horizons(layers['layer0']['scores'], layers['layer1']['scores'], layers['layer2']['scores'],\n",
    "                          score = \"MAE\", threshold = tolerance, hod=None, log_y=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdee697",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'myevalenv (Python 3.12.6)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def evaluate(observations, \n",
    "             fc_numerical, \n",
    "             fc_emulators,\n",
    "             score, \n",
    "             maximum_leadtime):\n",
    "    \n",
    "    layers = {}\n",
    "    for layer in [0,1,2]:\n",
    "        EvaluateModel = PointEvaluation(score =  score,\n",
    "                                    layer_index = layer,\n",
    "                                    variable_indices = matching_indices,\n",
    "                                    maximum_evaluation_time = maximum_leadtime)\n",
    "        scores = {}\n",
    "        skill_scores = {}\n",
    "        for mod, fc_emulator in fc_emulators.items():\n",
    "\n",
    "            EvaluateModel.set_samples(observations=observations,\n",
    "                                    fc_numerical=fc_numerical,\n",
    "                                    fc_emulator=fc_emulator)\n",
    "            EvaluateModel.subset_samples()\n",
    "            scores[\"ecland\"] = EvaluateModel.evaluate_numerical()\n",
    "            scores[mod] = EvaluateModel.evaluate_emulator()\n",
    "            skill_scores[mod] = EvaluateModel.get_skill_score()\n",
    "            \n",
    "        layers[f\"layer{layer}\"] = {}\n",
    "        layers[f\"layer{layer}\"][\"scores\"] = scores\n",
    "        layers[f\"layer{layer}\"][\"skill_scores\"] = skill_scores\n",
    "\n",
    "    return layers\n",
    "\n",
    "layers = evaluate(observations=station_data,\n",
    "                  fc_numerical=fc_numerical,\n",
    "                  fc_emulators=fc_emulators,\n",
    "                  score=score,\n",
    "                  maximum_leadtime=maximum_leadtime)\n",
    "\n",
    "layers_h1 = evaluate(observations=station_data_1,\n",
    "                  fc_numerical=fc_numerical_1,\n",
    "                  fc_emulators=fc_emulators_1,\n",
    "                  score=score,\n",
    "                  maximum_leadtime=maximum_leadtime)\n",
    "\n",
    "layers_h2 = evaluate(observations=station_data_2,\n",
    "                  fc_numerical=fc_numerical_2,\n",
    "                  fc_emulators=fc_emulators_2,\n",
    "                  score=score,\n",
    "                  maximum_leadtime=maximum_leadtime)\n",
    "layers_h3 = evaluate(observations=station_data_3,\n",
    "                  fc_numerical=fc_numerical_3,\n",
    "                  fc_emulators=fc_emulators_3,\n",
    "                  score=score,\n",
    "                  maximum_leadtime=maximum_leadtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae91e43",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'myevalenv (Python 3.12.6)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "GevenichPlots.plot_scores(layers['layer0']['scores'], layers['layer1']['scores'], layers['layer2']['scores'],\n",
    "                          score = \"MAE\", log_y=False)\n",
    "GevenichPlots.plot_skill_scores(layers['layer0']['skill_scores'], layers['layer1']['skill_scores'], layers['layer2']['skill_scores'],\n",
    "                          score = \"MAE\", log_y=False)\n",
    "\n",
    "GevenichPlots.plot_scores(layers_h1['layer0']['scores'], layers_h1['layer1']['scores'], layers_h1['layer2']['scores'],\n",
    "                          score = \"MAE\",hod=1, log_y=False)\n",
    "GevenichPlots.plot_skill_scores(layers_h1['layer0']['skill_scores'], layers_h1['layer1']['skill_scores'], layers_h1['layer2']['skill_scores'],\n",
    "                          score = \"MAE\",hod=1, log_y=False)\n",
    "\n",
    "GevenichPlots.plot_scores(layers_h3['layer0']['scores'], layers_h3['layer1']['scores'], layers_h3['layer2']['scores'],\n",
    "                          score = \"MAE\",hod=3, log_y=False)\n",
    "GevenichPlots.plot_skill_scores(layers_h3['layer0']['skill_scores'], layers_h3['layer1']['skill_scores'], layers_h3['layer2']['skill_scores'],\n",
    "                          score = \"MAE\",hod=3, log_y=False)\n",
    "\n",
    "GevenichPlots.plot_horizons(layers['layer0']['scores'], layers['layer1']['scores'], layers['layer2']['scores'],\n",
    "                          score = \"MAE\", threshold = tolerance, hod=None, log_y=False)\n",
    "GevenichPlots.plot_horizons(layers_h1['layer0']['scores'], layers_h1['layer1']['scores'], layers_h1['layer2']['scores'],\n",
    "                          score = \"MAE\", threshold = tolerance, hod=1, log_y=False)\n",
    "GevenichPlots.plot_horizons(layers_h3['layer0']['scores'], layers_h3['layer1']['scores'], layers_h3['layer2']['scores'],\n",
    "                          score = \"MAE\", threshold = tolerance, hod=3, log_y=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f78f75c",
   "metadata": {},
   "source": [
    "## Ensemble forecast with initial state perturbation\n",
    "\n",
    "The following steps are so far applicable only with the MLP. Here, we slightly perturb model forecast **at the first predicted time step after initialisation**, by a magnitude defined by sigma. This timestep is currently $t_0$ for the LSTM which estimates initial conditions from the lookback, but it is $t_1$ for the MLP and XGB that are initialised with observations at $t_0$. We run multiple deterministic forecasts with this disturbance (defined as ensemble here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083f3a33",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'myevalenv (Python 3.12.6)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def perturb_initial_state(initial_state, perturbation_factor = 0.01):\n",
    "    random_perturbation = torch.rand(initial_state.size()) * 2 - 1  # random values between -1 and 1\n",
    "    return initial_state * (1 + perturbation_factor * random_perturbation)\n",
    "\n",
    "def create_perturbation_ensemble(initial_state, size = 50):\n",
    "    torch.manual_seed(42)\n",
    "    perturbation_ensemble = [perturb_initial_state(initial_state) for i in range(size)]\n",
    "    return perturbation_ensemble\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef57d6b",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'myevalenv (Python 3.12.6)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "dynamic_features_ensemble = {}\n",
    "dynamic_features_prediction_ensemble = {}\n",
    "\n",
    "for mod in [\"mlp\", \"lstm\", \"xgb\"]:\n",
    "\n",
    "    if mod == 'mlp':\n",
    "        print('mlp')\n",
    "        CONFIG = load_config(config_path = '../../configs/mlp_emulator.yaml')\n",
    "        HPARS = load_hpars(use_model = '../mlp')\n",
    "        ForecastModel = ForecastModuleMLP(hpars=HPARS, config=CONFIG)    \n",
    "    elif mod == 'lstm':\n",
    "        CONFIG = load_config(config_path = '../../configs/lstm_emulator.yaml')\n",
    "        HPARS = load_hpars(use_model = '../lstm')\n",
    "        ForecastModel = ForecastModuleLSTM(hpars=HPARS, config=CONFIG)\n",
    "    elif mod == 'xgb':\n",
    "        CONFIG = load_config(config_path = '../../configs/xgb_emulator.yaml')\n",
    "        HPARS = None\n",
    "        ForecastModel = ForecastModuleXGB(hpars=HPARS, config=CONFIG)\n",
    "\n",
    "    CONFIG['x_slice_indices'] = closest_grid_cell # adjust the index of the grid cell in the config file before initialising the models\n",
    "\n",
    "    dataset = ForecastModel.initialise_dataset()\n",
    "    model = ForecastModel.load_model()\n",
    "    x_static, x_met, y_prog, y_prog_initial_state = ForecastModel.load_test_data(dataset)  \n",
    "    print(\"INITIAL STATE SHAPE:\", y_prog_initial_state.shape)\n",
    "\n",
    "    station_data = Gevenich.slice_station_data(lookback=CONFIG[\"lookback\"],\n",
    "                                    t_0=EX_CONFIG['initial_time'])\n",
    "    matching_indices = Gevenich.match_indices(dataset=dataset,\n",
    "                                              target_variables=EX_CONFIG['targets_eval'])\n",
    "    y_prog_initial_state[..., matching_indices] = station_data[0]\n",
    "    \n",
    "    perturbed_ensemble = create_perturbation_ensemble(y_prog_initial_state)\n",
    "\n",
    "    ensemble_prediction = []\n",
    "    for i in range(len(perturbed_ensemble)):\n",
    "        matching_indices = Gevenich.match_indices(dataset=dataset,\n",
    "                                              target_variables=EX_CONFIG['targets_prog'])\n",
    "        initial_vector =  Gevenich.transform_station_data(station_data = perturbed_ensemble[i])\n",
    "\n",
    "        dynamic_features, dynamic_features_prediction = ForecastModel.run_forecast(initial_conditions=initial_vector, \n",
    "                                                                                initial_conditions_perturbation=None,\n",
    "                                                                                predictions_perturbation = None)\n",
    "        dynamic_features, dynamic_features_prediction = ForecastModel.backtransformation()\n",
    "        ensemble_prediction.append(dynamic_features_prediction)\n",
    "\n",
    "    dynamic_features_ensemble[mod] = dynamic_features\n",
    "    dynamic_features_prediction_ensemble[mod] = ensemble_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18974e06",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'myevalenv (Python 3.12.6)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "dynamic_features_prediction_ensemble['mlp'][0].shape\n",
    "dynamic_features_prediction_ensemble['lstm'][0].shape\n",
    "dynamic_features_prediction_ensemble['xgb'][0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb6aaa3",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'myevalenv (Python 3.12.6)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "ensemble_dict = dynamic_features_prediction_ensemble\n",
    "for key, ensemble_prediction in ensemble_dict.items():\n",
    "    ensemble_dict[key] = np.stack(ensemble_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92dba58",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'myevalenv (Python 3.12.6)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "size = EX_CONFIG['ensemble_size']\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5), sharex=True, sharey=True) \n",
    "\n",
    "colors = [\"magenta\", \"purple\", \"pink\"]\n",
    "j=0\n",
    "for key, ensemble_prediction in ensemble_dict.items():\n",
    "    for i in range(size):\n",
    "        label = key if i == 0 else None\n",
    "        ax[0].plot(ensemble_prediction[i, :,:,3], color=colors[j], label=label, alpha = 0.5)\n",
    "    j+=1\n",
    "ax[0].plot(dynamic_features_ensemble['lstm'][:,:,3], color=\"darkblue\", label=\"ecland\", alpha = 0.9)\n",
    "ax[0].plot(station_data[:,:,0], color=\"green\", label=\"station data\", alpha = 0.9)\n",
    "ax[0].set_title(\"Layer 1\")\n",
    "ax[0].legend()\n",
    "\n",
    "j=0\n",
    "for key, ensemble_prediction in ensemble_dict.items():\n",
    "    for i in range(size):\n",
    "        label = key if i == 0 else None\n",
    "        ax[1].plot(ensemble_prediction[i, :,:,4], color=colors[j], label=label, alpha = 0.5)\n",
    "    j+=1\n",
    "ax[1].plot(dynamic_features_ensemble['lstm'][:,:,4], color=\"darkblue\", label=\"ecland\", alpha = 0.9)\n",
    "ax[1].plot(station_data[:,:,1], color=\"green\", label=\"station data\", alpha = 0.9)\n",
    "ax[1].set_title(\"Layer 2\")\n",
    "ax[1].legend()\n",
    "\n",
    "j=0\n",
    "for key, ensemble_prediction in ensemble_dict.items():\n",
    "    for i in range(size):\n",
    "        label = key if i == 0 else None\n",
    "        ax[2].plot(ensemble_prediction[i, :,:,5], color=colors[j], label=label, alpha = 0.5)\n",
    "    j+=1\n",
    "ax[2].plot(dynamic_features_ensemble['lstm'][:,:,5], color=\"darkblue\", label=\"ecland\", alpha = 0.9)\n",
    "ax[2].plot(station_data[:,:,2], color=\"green\", label=\"station data\", alpha = 0.9) \n",
    "ax[2].set_title(\"Layer 3\")\n",
    "ax[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "fig_path = os.path.join(path_to_plots, f'{network.split('_')[1]}_{station}_{2022}_{variable}_ensemble_forecast.pdf')\n",
    "plt.savefig(fig_path)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
